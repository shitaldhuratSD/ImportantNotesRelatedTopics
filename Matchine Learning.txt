

			** Machine Learning **

https://www.freecodecamp.org/news/how-to-learn-machine-learning-practical-tips-and-resources/
https://www.youtube.com/watch?v=GwIo3gDZCVQ
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
There are multiple definitions for Machine Learning by different people, like:

In 1959, Arthur Samuel, a computer scientist who pioneered the study of artificial intelligence, described machine learning as “the study that gives computers the ability to learn without being explicitly programmed.”
Alan Turing’s seminal paper (Turing, 1950) introduced a benchmark standard for demonstrating machine intelligence, such that a machine has to be intelligent and responsive in a manner that cannot be differentiated from that of a human being.
A more technical definition given by Tom M. Mitchell’s (1997) : “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.”
Generally speaking, Machine Learning is an application of artificial intelligence where a computer/machine learns from the past experiences (input data) and makes future predictions. The performance of such a system should be at least human level.image.png

Applications of Machine Learning
Web search: ranking page based on what you are most likely to click on.
Computational biology: rational design drugs in the computer based on past experiments.
Finance: decide who to send what credit card offers to. Evaluation of risk on credit offers. How to decide where to invest money.
E-commerce: Predicting customer churn. Whether or not a transaction is fraudulent.
Space exploration: space probes and radio astronomy.
Robotics: how to handle uncertainty in new environments. Autonomous. Self-driving car.
Information extraction: Ask questions over databases across the web.
Social networks: Data on relationships and preferences. Machine learning to extract value from data.
Debugging: Use in computer science problems like debugging. Labor intensive process. Could suggest where the bug could be.
Key Elements of Machine Learning
Every machine learning algorithm has three components:

Representation: how to represent knowledge. Examples include decision trees, sets of rules, instances, graphical models, neural networks, support vector machines, model ensembles and others.
Evaluation: the way to evaluate candidate programs (hypotheses). Examples include accuracy, prediction and recall, squared error, likelihood, posterior probability, cost, margin, entropy k-L divergence and others.
Optimization: the way candidate programs are generated known as the search process. For example combinatorial optimization, convex optimization, constrained optimization.
Steps used in Machine Learning
There are 5 basic steps used to perform a machine learning task:

Collecting data: Be it the raw data from excel, access, text files etc., this step (gathering past data) forms the foundation of the future learning. The better the variety, density and volume of relevant data, better the learning prospects for the machine becomes.
Preparing the data: Any analytical process thrives on the quality of the data used. One needs to spend time determining the quality of data and then taking steps for fixing issues such as missing data and treatment of outliers. Exploratory analysis is perhaps one method to study the nuances of the data in details thereby increasing the valuable content of the data.
Training a model: This step involves choosing the appropriate algorithm and representation of data in the form of the model. The cleaned data is split into two parts – train and test (proportion depending on the prerequisites); the first part (training data) is used for developing the model. The second part (test data), is used as a reference.
Evaluating the model: To test the accuracy, the second part of the data (holdout / test data) is used. This step determines the precision in the choice of the algorithm based on the outcome. A better test to check accuracy of model is to see its performance on data which was not used at all during model build.
Improving the performance: This step might involve choosing a different model altogether or introducing more variables to augment the efficiency. That’s why significant amount of time needs to be spent in data collection and preparation.
Types of Machine Learning
Machine Learning is generally categorized into three types:

Supervised Learning,
Unsupervised Learning,
Reinforcement learning
image.png

Supervised Learning:
Supervised Learning is the first type of machine learning, in which labelled data used to train the algorithms. In supervised learning, algorithms are trained using marked data, where the input and the output are known. We input the data in the learning algorithm as a set of inputs, which is called as Features, denoted by X along with the corresponding outputs, which is indicated by Y, and the algorithm learns by comparing its actual production with correct outputs to find errors. It then modifies the model accordingly. The raw data divided into two parts. The first part is for training the algorithm, and the other region used for test the trained algorithm. Supervised learning uses the data patterns to predict the values of additional data for the labels. This method will commonly use in applications where historical data predict likely upcoming events. Ex:- It can anticipate when transactions are likely to be fraudulent or which insurance customer is expected to file a claim.

image.png

Two of the most common supervised machine learning tasks are classification and regression.

image.png

In classification problems the machine must learn to predict discrete values. That is, the machine must predict the most probable category, class, or label for new examples. Applications of classification include predicting whether a stock's price will rise or fall, or deciding if a news article belongs to the politics or leisure section.

In regression problems the machine must predict the value of a continuous response variable. Regression is a form of predictive modelling technique which investigates the relationship between a dependent variable**(Outputs)** and independent variable**(Inputs)**. Examples of regression problems include predicting the sales for a new product, or the salary for a job based on its description, forecasting the weather, time series modelling, process optimisation. One of the examples of the regression technique is House Price Prediction, where the price of the house will predict from the inputs such as No of rooms, Locality, Ease of transport, Age of house, Area of a home.

Unsupervised Learning:
Unsupervised Learning is the second type of machine learning, in which unlabeled data are used to train the algorithm, which means it used against data that has no historical labels. What is being shown must be figured out by the algorithm. The purpose is to explore the data and find some structure within. In unsupervised learning the data is unlabeled. The algorithm figures out the data and according to the data segments, it makes clusters of data with new labels. This learning technique works well on transactional data. For example, it can identify segments of customers with similar attributes who can then be treated similarly in marketing campaigns. Or it can find the primary qualities that separate customer segments from each other. These algorithms are also used to segment text topics, recommend items and identify data outliers.

image.png

Reinforcement Learning:
Reinforcement Learning is the third type of machine learning in which no raw data is given as input instead reinforcement learning algorithm have to figures out the situation on their own. The reinforcement learning frequently used for robotics, gaming, and navigation. With reinforcement learning, the algorithm discovers through trial and error which actions yield the most significant rewards. This type of training has three main components which are the agent which can describe as the learner or decision maker, the environment which described as everything the agent interacts with and actions which represented as what the agent can do. The objective for the agent is to take actions that maximise the expected reward over a given measure of time. The agent will reach the goal much quicker by following a good policy. So the purpose of reinforcement learning is to learn the best plan. For example, maximize the points won in a game over many moves.

image.png

Why do we prefer Python to implement machine learning algorithms?
Python is a popular and general-purpose programming language. We can write machine learning algorithms using Python, and it works well. The reason why Python is so popular among data scientists is that Python has a diverse variety of modules and libraries already implemented that make our life more comfortable. Let us have a brief look at some exciting Python libraries.

Numpy: It is a math library to work with n-dimensional arrays in Python. It enables us to do computations effectively and efficiently.
Scipy: It is a collection of numerical algorithms and domain-specific tool-box, including signal processing, optimization, statistics, and much more. Scipy is a functional library for scientific and high-performance computations.
Matplotlib: It is a trendy plotting package that provides 2D plotting as well as 3D plotting.
Scikit-learn: It is a free machine learning library for python programming language. It has most of the classification, regression, and clustering algorithms, and works with Python numerical libraries such as Numpy, Scipy.
Important Terminologies
Accuracy - Percentage of correct predictions made by the model.

Algorithm - A method, function, or series of instructions used to generate a machine learning model. Examples include linear regression, decision trees, support vector machines, and neural networks.

Attribute - A quality describing an observation (e.g. color, size, weight).

Categorical Variables - Variables with a discrete set of possible values. Can be ordinal (order matters) or nominal (order doesn’t matter).

Confusion Matrix - Table that describes the performance of a classification model by grouping predictions into 4 categories.

True Positives: we correctly predicted they do have diabetes

True Negatives: we correctly predicted they don’t have diabetes

False Positives: we incorrectly predicted they do have diabetes (Type I error)

False Negatives: we incorrectly predicted they don’t have diabetes (Type II error)

Continuous Variables - Variables with a range of possible values defined by a number scale (e.g. sales, lifespan).

Convergence - A state reached during the training of a model when the loss changes very little between each iteration.

Deduction - A top-down approach to answering questions or solving problems. A logic technique that starts with a theory and tests that theory with observations to derive a conclusion. E.g. We suspect X, but we need to test our hypothesis before coming to any conclusions.

Dimension - Dimension for machine learning and data scientist is differ from physics, here Dimension of data means how much feature you have in you data-set. e.g in case of object detection application, flatten image size and color channel(e.g 28283) is a feature of the input set. In case of house price prediction (maybe) house size is the data-set so we call it 1 dimensional data.

Epoch - An epoch describes the number of times the algorithm sees the entire data set.

Feature - With respect to a dataset, a feature represents an attribute and value combination. Color is an attribute. “Color is blue” is a feature.

Feature Selection - Feature selection is the process of selecting relevant features from a data-set for creating a Machine Learning model.

Hyperparameters - Hyperparameters are higher-level properties of a model such as how fast it can learn (learning rate) or complexity of a model. The depth of trees in a Decision Tree or number of hidden layers in a Neural Networks are examples of hyper parameters.

Instance - A data point, row, or sample in a dataset. Another term for observation.

Label - The “answer” portion of an observation in supervised learning. For example, in a dataset used to classify flowers into different species, the features might include the petal length and petal width, while the label would be the flower’s species.

Learning Rate - The size of the update steps to take during optimization loops like Gradient Descent. With a high learning rate we can cover more ground each step, but we risk overshooting the lowest point since the slope of the hill is constantly changing. With a very low learning rate, we can confidently move in the direction of the negative gradient since we are recalculating it so frequently. A low learning rate is more precise, but calculating the gradient is time-consuming, so it will take us a very long time to get to the bottom.

Loss - The lower the loss, the better a model (unless the model has over-fitted to the training data). The loss is calculated on training and validation and its interpretation is how well the model is doing for these two sets. Unlike accuracy, loss is not a percentage. It is a summation of the errors made for each example in training or validation sets. Loss = true_value(from data-set) - predicted value(from ML-model)

Model - A data structure that stores a representation of a dataset (weights and biases). Models are created/learned when you train an algorithm on a dataset.

Neural Networks - Neural Networks are mathematical algorithms modeled after the brain’s architecture, designed to recognize patterns and relationships in data.

Noise - Any irrelevant information or randomness in a dataset which obscures the underlying pattern.

Observation - A data point, row, or sample in a dataset. Another term for instance.

Outlier - An observation that deviates significantly from other observations in the dataset.

Overfitting - Overfitting occurs when your model learns the training data too well and incorporates details and noise specific to your dataset. You can tell a model is overfitting when it performs great on your training/validation set, but poorly on your test set (or new real-world data).

Parameters - Parameters are properties of training data learned by training a machine learning model or classifier. They are adjusted using optimization algorithms and unique to each experiment.

Precision - In the context of binary classification (Yes/No), precision measures the model’s performance at classifying positive observations (i.e. “Yes”). In other words, when a positive value is predicted, how often is the prediction correct? We could game this metric by only returning positive for the single observation we are most confident in.

P = TruePositives / (TruePositives + FalsePositives)
Recall - Also called sensitivity. In the context of binary classification (Yes/No), recall measures how “sensitive” the classifier is at detecting positive instances. In other words, for all the true observations in our sample, how many did we “catch.” We could game this metric by always classifying observations as positive.

R = TruePositives / (TruePositives + FalseNegatives)
Regularization - Regularization is a technique utilized to combat the overfitting problem. This is achieved by adding a complexity term to the loss function that gives a bigger loss for more complex models.

Reinforcement Learning - Training a model to maximize a reward via iterative trial and error.

Test Set - A set of observations used at the end of model training and validation to assess the predictive power of your model. How generalizable is your model to unseen data?

Training Set - A set of observations used to generate machine learning models.

Transfer Learning - A machine learning method where a model developed for a task is reused as the starting point for a model on a second task. In transfer learning, we take the pre-trained weights of an already trained model (one that has been trained on millions of images belonging to 1000’s of classes, on several high power GPU’s for several days) and use these already learned features to predict new classes.

Underfitting - Underfitting occurs when your model over-generalizes and fails to incorporate relevant variations in your data that would give your model more predictive power. You can tell a model is underfitting when it performs poorly on both training and test sets.

Universal Approximation Theorem - A neural network with one hidden layer can approximate any continuous function but only for inputs in a specific range. If you train a network on inputs between -2 and 2, then it will work well for inputs in the same range, but you can’t expect it to generalize to other inputs without retraining the model or adding more hidden neurons.

Validation Set - A set of observations used during model training to provide feedback on how well the current parameters generalize beyond the training set. If training error decreases but validation error increases, your model is likely overfitting and you should pause training.

Variance - How tightly packed are your predictions for a particular observation relative to each other.

Low variance suggests your model is internally consistent, with predictions varying little from each other after every iteration.

High variance (with low bias) suggests your model may be overfitting and reading too deeply into the noise found in every training set.

visit for more - https://developers.google.com/machine-learning/glossary

Further Reading
Look into different supervised learning algorithms. Try to see if it can be used for classification and regression as well or not. Most commonly used algorithms are :
Linear Regression
Logistic Regression
K-Nearest Neighbors
Decision Tree
Random Forest
Support Vector Machine
https://towardsdatascience.com/metrics-to-evaluate-your-machine-learning-algorithm-f10ba6e38234
https://towardsdatascience.com/demystifying-optimizations-for-machine-learning-c6c6405d3eea
https://medium.com/towards-artificial-intelligence/machine-learning-algorithms-for-beginners-with-python-code-examples-ml-19c6afd60daa
https://towardsdatascience.com/machine-learning-for-beginners-d247a9420dab